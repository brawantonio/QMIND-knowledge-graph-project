{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96737097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import file2doc, doc2vec\n",
    "\n",
    "#import preprocessing\n",
    "#from nltk.corpus import stopwords\n",
    "from rake_nltk import Rake\n",
    "import yake\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec #,TaggedDocument\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43f3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is model dependent -------------------------------------\n",
    "#stop = stopwords.words(\"english\")\n",
    "#language = \"en\"\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## replace this bit with dialogue box\n",
    "#dirpath = \"/Users/braulioantonio/Documents/Python/Hazelnut/datasets/qmind_linkedin/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25084f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = file2doc.get_dirpath()\n",
    "files_dictionary = file2doc.get_files(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ef1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = doc2vec.corpus_dataframe(dirpath, files_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3bd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# this will be in a future function in doc2vec -----------------------------------\n",
    "corpus_data = []\n",
    "for key in files_dictionary.keys():\n",
    "    for file in files_dictionary[key]:\n",
    "        file_metadata = file2doc.metadata(dirpath + file)\n",
    "        corpus_data.append(file2doc.extract_text(dirpath+file, key) + file_metadata.all())\n",
    "\n",
    "clean_corpus_data = [preprocessing.stdtextpreprocessing(element) for element in corpus_data]\n",
    "# Extract title of project\n",
    "titles = [text.partition(\"\\n\")[0] for text in corpus_data]\n",
    "# Delete title from each element in data\n",
    "for i in range(len(corpus_data)):\n",
    "    corpus_data[i] = corpus_data[i].replace(titles[i], \"\")\n",
    "\n",
    "# Create df\n",
    "df = pd.DataFrame(list(zip(titles, corpus_data)),columns =['title', 'desc'])\n",
    "# Create text column\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"desc\"]\n",
    "# Display\n",
    "#df\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocessing.stdtextpreprocessing)\n",
    "#df = df[[\"title\", \"clean_text\"]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6f6c2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7e200bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS MAY NOT BE NEEDED ### THIS WILL BE REPLACED BY THE QUERY / SEARCH ENGINE\n",
    "rake_nltk_var = Rake() #model to extract phrases/keywords\n",
    "\n",
    "rake_nltk_var.extract_keywords_from_text(df[\"clean_text\"][2])\n",
    "keyword_extracted = rake_nltk_var.get_ranked_phrases()[:8] #long sentences here\"\"\"\n",
    "\n",
    "### THIS MAY NOT BE NEEDED\n",
    "language = \"en\"\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 1\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "\n",
    "\n",
    "tokenized_sentences = []\n",
    "tags = []\n",
    "for text in keyword_extracted:\n",
    "    #text = keyword_extracted[0] #I need to iterate this one\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    #tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    tokenized_sentences.append([nltk.word_tokenize(sent) for sent in sentences][0]) #in the future there might be more than one sentence here\n",
    "    #text = keyword_extracted[0] I could replace this with name extraction\n",
    "    #keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    tags.append(custom_kw_extractor.extract_keywords(text)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a7a93",
   "metadata": {},
   "source": [
    "### Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d9c1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(os.getcwd() + '/hazelnut_model') \n",
    "tsne = TSNE(n_components=2, learning_rate='auto',init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0756291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj2vec = [model.infer_vector(token) for token in tokenized_sentences]\n",
    "proj2vec = np.asarray(proj2vec)\n",
    "# Visualize Vectors\n",
    "#tsne.fit_transform(proj2vec)\n",
    "#proj2vec.shape\n",
    "#proj2vec = model.infer_vector(tokenized_sentences[0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b6a587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_sentences\n",
    "#tsne.fit_transform(proj2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6ae5091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2de7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22ede859",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce838d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 1 sample(s) (shape=(1, 64)) while a minimum of 2 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v0/ytm6tkws64dc456ps8f0dlwc0000gn/T/ipykernel_4040/621719853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 2 d projection and plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/hazelnut/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \"\"\"\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/hazelnut/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m             )\n\u001b[1;32m    836\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/hazelnut/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/hazelnut/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m             )\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 1 sample(s) (shape=(1, 64)) while a minimum of 2 is required."
     ]
    }
   ],
   "source": [
    "## 2 d projection and plotting\n",
    "x = tsne.fit_transform(proj2vec)[:,0]\n",
    "y = tsne.fit_transform(proj2vec)[:,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18,10) \n",
    "ax.scatter(x, y, s=5000)\n",
    "ax.set_axis_off()\n",
    "\n",
    "for i, txt in enumerate(tags):\n",
    "    ax.annotate(txt, (x[i], y[i]), size = 10)\n",
    "fig.savefig('/Users/braulioantonio/Documents/Python/Hazelnut/demo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b9be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08767884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6d7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f78c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1eeacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bd0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540daab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e5046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf89a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a24c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9deb4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58839c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d9632c9",
   "metadata": {},
   "source": [
    "### Attempt to get key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a23738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yake\n",
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "56021974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('QMIND Design Directors', 0.0034087743786865242)\n",
      "('Design', 0.003803367620959844)\n",
      "('model', 0.006026683858421884)\n",
      "('Data', 0.01037179048069565)\n",
      "('Hospital', 0.01494021983163719)\n",
      "('QMIND', 0.015431246177648761)\n",
      "('team', 0.016851979147917674)\n",
      "('Project', 0.018328737124292532)\n",
      "('neural network', 0.02143876288483478)\n",
      "('learning', 0.024469086009800935)\n",
      "('Human', 0.024762152836234168)\n",
      "('work', 0.02477571025298345)\n",
      "('Apple Watch Data', 0.03007656525869324)\n",
      "('accuracy', 0.03075747031564878)\n",
      "('Information', 0.031181533435243623)\n"
     ]
    }
   ],
   "source": [
    "text = corpus_data[2]\n",
    "language = \"en\"\n",
    "\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.3\n",
    "numOfKeywords = 15\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f309c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5b2a252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look forward meeting team member diving deeper project month meanwhile great time past weekend attending camp qmind two day onboarding event design team member learned lot got inspired speaker fellow colleague workshop excellent platform beginner understand basic computer based model working datasets ml algorithm big shoutout tomax bennett sydney corbettand team organizing event really tuned mindset changing world ai ai dataengineering healthcare see', 'top form bottom form natela urushadzeout network rd ac coordinator tbilisi youth palace w qmind follower mo day left apply qmind director year check last year qmind director travis cossariniandtania sidhom say experience role theyve gained member director team great opportunity gain unbelievable leadership experience undergraduate level work alongside extremely talented driven student industry professional cutting edge tech field dont miss', 'qmind follower mo edited interested learning ai shape future rapidly advancing technology key player shaping future world join u saturday april pm est hear three knowledgeable expert explore use cutting edge ai ml technology realm sustainable development clean energy cancer research short time problem topic presented minute speed workshop feel free pop please dm u question register advance', 'document finished loading max bennett others aine nuraizza nuruddin stprotection control metering engineering intern hydro one rd year electrical engineering certificate business queen su cigre canada ngn exec member ospe student member mo thrilled announce part ofqmindthis year member canada largest undergraduate ai machine learning hub honour got selected work client facing consulting team calledthe fertility partner', 'main goal bridge gap student endless talent tech industry set success career development journey far qmind pd showcased student work big tech recruitment process want introduce student work start ups tech start ups bring innovative solution world hear exciting update progress news time join u march pm est hear esteemed line panelist work startup']\n"
     ]
    }
   ],
   "source": [
    "rake_nltk_var = Rake()\n",
    "text = clean_corpus_data[0]\n",
    "\n",
    "rake_nltk_var.extract_keywords_from_text(text)\n",
    "keyword_extracted = rake_nltk_var.get_ranked_phrases()[:5]\n",
    "print(keyword_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c8669",
   "metadata": {},
   "source": [
    "## Applying the models and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c36591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model = Doc2Vec.load(os.getcwd() + '/hazelnut_model') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj2vec = [model.infer_vector((df['clean_text'][i].split(' '))) for i in range(0,len(df['clean_text']))]\n",
    "# Display\n",
    "#proj2vec\n",
    "df['proj2vec'] = np.array(proj2vec).tolist()\n",
    "# Display\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50894cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"clean_text\"][0]\n",
    "#document = \" \".join([i for i in clean_corpus_data[0].split() if i not in stop])\n",
    "#sentences = nltk.sent_tokenize(document)\n",
    "#sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "#len(keyword_extracted)\n",
    "#print(keyword_extracted)\n",
    "#document = \" \".join([i for i in keyword_extracted[0].split() if i not in stop])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
